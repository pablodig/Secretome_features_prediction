{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12b1882-9aff-414d-b57e-1f5df9761e60",
   "metadata": {},
   "source": [
    "Regression Pipeline\n",
    "===================\n",
    "\n",
    "This script demonstrates how to preprocess data and run an Optuna study to tune hyperparameters \n",
    "for a regression model. The features are sourced from 'media-4.xlsx' (using the sheet 'all features')\n",
    "while the target values (viability) are loaded from 'secretome_samples_for_RNAseq_170111.xlsx'.\n",
    "\n",
    "The viability target is a continuous variable, so we modify the pipeline as follows:\n",
    "  - Load the target from a new Excel file and select the columns ['Gene name', 'Viability (harvest) [%]'].\n",
    "  - Rename columns so that the gene names match the feature dataframe index and rename the viability column to “Viability”.\n",
    "  - Remove any sampling or stratification (since the target is continuous).\n",
    "  - In the Optuna objective, choose among regression models and use an appropriate metric (R²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c60a9b-b819-4c09-8325-6d6b432e3bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Import Libraries & Modules\n",
    "#############################\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Data splitting and imputation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ML models\n",
    "import shap\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfef23-670e-4e50-a8f5-7abe322f06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Define Helper Functions\n",
    "###########################\n",
    "\n",
    "def show_heatmap(df, predictor_list, target='Viability', figsize=(20, 10), title='Correlation Heatmap'):\n",
    "    # Create a correlation matrix (only numeric columns) and plot a heatmap\n",
    "    correlation_matrix = df[predictor_list + [target]].corr(method='pearson')\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm', center=0, linewidths=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def standardize_values(X):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    return scaler.transform(X)\n",
    "\n",
    "def t_sne_plot(X, y, title='t-SNE plot'):\n",
    "    from sklearn.manifold import TSNE\n",
    "    X_tsne = TSNE(n_components=2, random_state=0).fit_transform(X)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Viability')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b741b1-fbc6-4b1e-a244-9c735cea33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Load & Preprocess Features\n",
    "###########################\n",
    "\n",
    "# Read the features file (same as before)\n",
    "df = pd.read_excel('data/media-4.xlsx', sheet_name='all features')\n",
    "\n",
    "# Drop columns that are not needed and set the index\n",
    "df = df.drop(columns=['sample_ID','uniprot_id'])\n",
    "df.set_index('human_symbol', inplace=True)\n",
    "\n",
    "# Group by index (to average out duplicates, if any)\n",
    "df = df.groupby(df.index).mean()\n",
    "\n",
    "# Visualize missing values\n",
    "msno.bar(df, figsize=(12, 48), sort=\"ascending\", fontsize=12, color='tomato')\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "threshold = 0.5\n",
    "missing_percentage = df.isnull().mean()\n",
    "columns_to_drop = missing_percentage[missing_percentage > threshold].index\n",
    "df_dropped = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Impute remaining missing values using KNNImputer\n",
    "original_index = df_dropped.index\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_imputed_array = imputer.fit_transform(df_dropped)\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=df_dropped.columns, index=original_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec20bdf-b7c8-4953-9796-10e239ab7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Load & Preprocess the Target\n",
    "###############################\n",
    "\n",
    "# Read the new target file for regression (viability)\n",
    "target_df = pd.read_excel('data/secretome_samples_for_RNAseq_170111.xlsx', sheet_name='all')\n",
    "\n",
    "# Select only the relevant columns\n",
    "target_df = target_df[['Gene name', 'Viability (harvest) [%]']]\n",
    "\n",
    "# Rename columns so that the gene names match the features dataframe index\n",
    "target_df = target_df.rename(columns={'Gene name': 'human_symbol', \n",
    "                                      'Viability (harvest) [%]': 'Viability'})\n",
    "\n",
    "# Set the index to 'human_symbol'\n",
    "target_df.set_index('human_symbol', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5a30a-e600-4eee-8ff2-47283b746881",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Merge Features with the Regression Target\n",
    "######################################\n",
    "\n",
    "# Join the imputed features with the target viability values\n",
    "df_merged = df_imputed.join(target_df, how='inner')\n",
    "print(\"Merged data shape:\", df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98a1a8-3117-4fd3-9e6b-47bb615abc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Explore Balance, Variability, and Correlations\n",
    "#############################################\n",
    "\n",
    "# Identify any columns with zero variability (if any)\n",
    "zero_var_cols = df_merged.columns[df_merged.nunique() == 1]\n",
    "print(\"Zero-variability columns:\", zero_var_cols)\n",
    "\n",
    "# Identify and drop highly correlated features (threshold = 0.80)\n",
    "correlation_threshold = 0.80\n",
    "correlation_matrix = df_merged.corr(method='pearson')\n",
    "upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > correlation_threshold)]\n",
    "print(f'{len(to_drop)} columns to drop due to high correlation:', to_drop)\n",
    "\n",
    "# Create a list of predictor columns by excluding the target and dropped columns\n",
    "predictor_list = list(df_merged.columns)\n",
    "predictor_list.remove('Viability')\n",
    "predictor_list = [x for x in predictor_list if x not in to_drop]\n",
    "print(\"Number of predictors:\", len(predictor_list))\n",
    "\n",
    "# (Optional) Save the predictor list\n",
    "with open('preprocessed/predictor_list_regression.json', 'w') as f:\n",
    "    json.dump(predictor_list, f)\n",
    "\n",
    "# Visualize correlations (features vs. viability)\n",
    "show_heatmap(df=df_merged, target='Viability', predictor_list=predictor_list, \n",
    "             title='Correlation Heatmap (Viability)', figsize=(65,65))\n",
    "\n",
    "# (Optional) Standardize values and explore with t-SNE\n",
    "X_stand  = standardize_values(df_merged[predictor_list].values)\n",
    "t_sne_plot(X_stand, df_merged['Viability'].values, title='t-SNE plot of standardized features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6c0ce-098e-4985-9fe4-61265c694f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Data Splitting Strategy (Regression)\n",
    "######################################\n",
    "\n",
    "# Note: When dealing with a continuous target, stratification is not applicable.\n",
    "# We split the merged data into training, validation, calibration, and testing sets.\n",
    "\n",
    "# Step 1: Split into training (70%) and temporary (30%) sets\n",
    "train_df, tmp_df = train_test_split(df_merged, test_size=0.3, random_state=1)\n",
    "\n",
    "# Step 2: Split temporary set into validation (20% of original) and testing (10% of original)\n",
    "tmp2_df, test_df = train_test_split(tmp_df, test_size=0.33, random_state=1)\n",
    "\n",
    "# Step 3: Split tmp2_df into validation (10% of original) and calibration (10% of original)\n",
    "val_df, cal_df = train_test_split(tmp2_df, test_size=0.5, random_state=1)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Calibration shape:\", cal_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a97a5-adf0-4e83-9544-9fc82754ca94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Optuna Study for Regression\n",
    "######################################\n",
    "\n",
    "# Helper function to get training data (features and continuous target)\n",
    "def get_data():\n",
    "    y = train_df['Viability'].values.astype(float)\n",
    "    X = train_df[predictor_list].values\n",
    "    return X, y\n",
    "\n",
    "# Define the objective function for regression.\n",
    "# We use the coefficient of determination (R²) as the evaluation metric.\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    X, y = get_data()\n",
    "    # Split training data into a new training and validation set (no stratification)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "    # Suggest model type: choose among four regression approaches.\n",
    "    model_type = trial.suggest_categorical(\"model\", [\"XGBoost\", \"RandomForest\", \"Ridge\"])\n",
    "    \n",
    "    if model_type == \"XGBoost\":\n",
    "        # XGBoost regressor parameters\n",
    "        param = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.1),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 1, 12),\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0)\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**param)\n",
    "        model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=False)\n",
    "        preds = model.predict(valid_x)\n",
    "        \n",
    "    elif model_type == \"RandomForest\":\n",
    "        # RandomForest regressor parameters\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 1, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "        }\n",
    "        model = RandomForestRegressor(**param)\n",
    "        model.fit(train_x, train_y)\n",
    "        preds = model.predict(valid_x)\n",
    "        \n",
    "    elif model_type == \"Ridge\":\n",
    "        # Ridge regression hyperparameter: alpha (regularization strength) and whether to fit an intercept.\n",
    "        alpha = trial.suggest_float(\"ridge_alpha\", 1e-6, 1e2, log=True)\n",
    "        fit_intercept = trial.suggest_categorical(\"ridge_fit_intercept\", [True, False])\n",
    "        model = Ridge(alpha=alpha, fit_intercept=fit_intercept, max_iter=1000)\n",
    "        model.fit(train_x, train_y)\n",
    "        preds = model.predict(valid_x)\n",
    "    \n",
    "    # Evaluate the model using R² (the higher, the better)\n",
    "    score = r2_score(valid_y, preds)\n",
    "    return score\n",
    "\n",
    "# Create an Optuna study with the goal of maximizing R²\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "    direction=\"maximize\"\n",
    ")\n",
    "study.optimize(objective, n_trials=500, timeout=3600)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  R² Value: {:.4f}\".format(best_trial.value))\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\"Best parameters saved to best_params_regression.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384ec1-6f64-43e1-89d4-11a0506fed6c",
   "metadata": {},
   "source": [
    "#### Final Model Training and SHAP Analysis for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daccc14-227d-4cca-9d34-e312cf7f95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae66946-7510-4b8e-ba68-bc70a43695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# (1) Combine the Datasets\n",
    "#############################\n",
    "# Combine training, validation, calibration, and test splits into one full dataset.\n",
    "full_df = pd.concat([train_df, val_df, cal_df], ignore_index=True)\n",
    "print(\"Full dataset shape:\", full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9d8f5-e256-4053-85e0-49f546804249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# (2) Prepare Data for Training\n",
    "#############################\n",
    "X_full = full_df[predictor_list].values\n",
    "y_full = full_df[\"Viability\"].values  # Continuous target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3eed3e-aebb-4612-8c58-c3d17f127d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# (3) Train the Best XGBoost Regression Model\n",
    "#############################\n",
    "# Set up parameters for XGBoost using the best hyperparameters.\n",
    "# (Make sure the objective is \"reg:squarederror\" for regression.)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"colsample_bytree\": best_trial.params[\"xgb_colsample_bytree\"],\n",
    "    \"learning_rate\": best_trial.params[\"xgb_learning_rate\"],\n",
    "    \"max_depth\": best_trial.params[\"xgb_max_depth\"],\n",
    "    \"n_estimators\": best_trial.params[\"xgb_n_estimators\"],\n",
    "    \"subsample\": best_trial.params[\"xgb_subsample\"]\n",
    "}\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "\n",
    "# Train on the entire dataset\n",
    "print(\"Training the XGBoost regressor on the full dataset...\")\n",
    "model.fit(X_full, y_full)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d98c5-d314-41b1-8620-c6bd5de2bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# (4) Evaluate on Test Set\n",
    "#############################\n",
    "\n",
    "X_test = test_df[predictor_list].values\n",
    "y_test = test_df[\"Viability\"].values\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, preds)\n",
    "print(f\"XGBoost Regression model R² on test set: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24403127-9045-4822-a4b4-43accce32985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# (5) SHAP Analysis\n",
    "#############################\n",
    "\n",
    "# Initialize JS visualization (optional, for interactive plots in notebooks)\n",
    "shap.initjs()\n",
    "\n",
    "# Create a TreeExplainer for the trained model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP values on the test set (or you can choose a different subset)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "## (a) SHAP Beeswarm Summary Plot\n",
    "plt.figure()\n",
    "plt.title(\"SHAP Summary Plot (Beeswarm)\")\n",
    "shap.summary_plot(shap_values, X_test, feature_names=predictor_list)\n",
    "\n",
    "## (b) SHAP Bar Plot of Mean |SHAP Value|\n",
    "plt.figure()\n",
    "plt.title(\"SHAP Feature Importance (Bar Plot)\")\n",
    "shap.summary_plot(shap_values, X_test, feature_names=predictor_list, plot_type=\"bar\")\n",
    "\n",
    "\n",
    "print(\"SHAP analysis complete. You can view the interactive force plot in 'results/shap_force_plot.html'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77162996-354e-4200-ba64-d446fe5bf2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Compute Permutation Importance ---\n",
    "# Calculate permutation importance on the test set.\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=1)\n",
    "\n",
    "# Create a DataFrame for permutation importance.\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': predictor_list,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Step 2: Compute Pearson Correlations ---\n",
    "# Compute Pearson correlation coefficients between each feature and cell viability.\n",
    "# full_df is the complete dataset (all splits combined) that contains the \"Viability\" column.\n",
    "correlations = full_df[predictor_list + ['Viability']].corr()['Viability'].drop('Viability')\n",
    "\n",
    "# Add the correlation values to the permutation importance DataFrame.\n",
    "perm_df['Correlation'] = perm_df['Feature'].map(correlations)\n",
    "\n",
    "# --- Step 3: Create a Color Mapping Based on Correlation Sign ---\n",
    "# Green for features positively correlated with viability; red for negatively correlated.\n",
    "perm_df['Color'] = perm_df['Correlation'].apply(lambda x: 'green' if x > 0 else 'red')\n",
    "\n",
    "# --- Step 4: Plot the Top 10 Features with a Custom Legend ---\n",
    "# Subset to the top 10 most important features.\n",
    "top_features = perm_df.head(20)\n",
    "\n",
    "# --- Step 5: Define Mapping for Descriptive Feature Names ---\n",
    "# The keys here are substrings expected in the cleaned feature names.\n",
    "feature_mapping = {\n",
    "    'CTDT_CTDT.prop2.Tr1331': 'Transition volume: small/large',\n",
    "    'AA.comp_AA.Comp_W': 'AAC - Tryptophan',\n",
    "    'Peptides.R_charge_charge': 'Protein charge',\n",
    "    'AA.comp_AA.Comp_Q': 'AAC - Glutamine',\n",
    "    'abundance_PrEST_conc_cf': 'Production yield in yeast',\n",
    "    'CTDT_CTDT.prop7.Tr1331': 'Transition solvent accessibility: buried/intermediate',\n",
    "    'abundance_HPA_pancreas': 'Protein level in pancreas',\n",
    "    'CTDT_CTDT.prop6.Tr1221': 'Transition secondary structure: helix/strand',\n",
    "    'PSIM_NG': 'N-linked glycosylation',\n",
    "    'proStab_Tm.agg': 'Stability index',\n",
    "    'AA.comp_AA.Comp_A': 'AAC - Alanine',\n",
    "    'CTDT_CTDT.prop2.Tr1221': 'Transition volume: small/med',\n",
    "    'AA.comp_AA.Comp_N': 'AAC - Asparagine',\n",
    "    'CTDT_CTDT.prop4.Tr1221': 'Transition polarizability: low/med',\n",
    "    'AA.comp_AA.Comp_Y': 'AAC - Tyrosine',\n",
    "    'abundance_HPA_liver': 'Protein level in liver',\n",
    "    'Peptides.R_aaComp.mole_NonPolar_Mole%': 'Percent nonpolar residues',\n",
    "    'CTDT_CTDT.prop6.Tr2332': 'Transition secondary structure: strand/coil',\n",
    "    'CTDC_CTDC.hydrophobicity.Group3': 'AA comp - hydrophobic residues',\n",
    "    'CTDT_CTDT.prop7.Tr2332': 'Transition solvent accessibility: exposed/intermediate'\n",
    "}\n",
    "\n",
    "def map_feature_name(feature):\n",
    "    # Remove the \"sequence_\" prefix.\n",
    "    cleaned = feature.replace('sequence_', '')\n",
    "    # Remove a leading \"protr.\" if present.\n",
    "    if cleaned.startswith(\"protr.\"):\n",
    "        cleaned = cleaned[len(\"protr.\"):]\n",
    "    # Check if any key from our mapping appears in the cleaned feature.\n",
    "    for key, descriptive in feature_mapping.items():\n",
    "        if key in cleaned:\n",
    "            return descriptive\n",
    "    # If no mapping is found, return the cleaned name.\n",
    "    return cleaned\n",
    "\n",
    "# Apply the mapping function.\n",
    "top_features['Feature_mapped'] = top_features['Feature'].apply(map_feature_name)\n",
    "\n",
    "# --- Step 6: Remap Colors to New Colors for Improved Aesthetics ---\n",
    "positive_color = 'royalblue'  # For positive correlation.\n",
    "negative_color = 'orange'     # For negative correlation.\n",
    "top_features['Color'] = top_features['Color'].apply(lambda x: positive_color if x == 'green' else negative_color)\n",
    "\n",
    "# --- Step 7: Plot with Custom Y-Axis Labels and Improved Aesthetics ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(\n",
    "    x='Importance', \n",
    "    y='Feature_mapped', \n",
    "    data=top_features, \n",
    "    palette=top_features['Color'].tolist(),\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Set title and labels with improved font sizes and bold styling.\n",
    "ax.set_title(\"Top 10 Permutation Importance with Correlation Direction\", fontsize=18, weight='bold')\n",
    "ax.set_xlabel(\"Permutation Importance\", fontsize=16)\n",
    "ax.set_ylabel(\"Feature\", fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# Add a vertical grid for the x-axis.\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "# Remove the top and right spines.\n",
    "sns.despine(ax=ax, top=True, right=True)\n",
    "\n",
    "# Create custom legend patches.\n",
    "positive_patch = mpatches.Patch(color=positive_color, label='Positive correlation with viability')\n",
    "negative_patch = mpatches.Patch(color=negative_color, label='Negative correlation with viability')\n",
    "plt.legend(handles=[positive_patch, negative_patch], loc='lower right', fontsize=14)\n",
    "\n",
    "plt.savefig(\"results/permutation_importance_ML_reg.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5fd74-025d-4e5b-aad8-84b3007d9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Seaborn theme for a clean and modern look.\n",
    "sns.set_theme(style='whitegrid', context='talk')\n",
    "\n",
    "# --- Step 1: Compute Pearson Correlations ---\n",
    "# Calculate Pearson correlation coefficients between each feature and cell viability.\n",
    "correlations = full_df[predictor_list + ['Viability']].corr()['Viability'].drop('Viability')\n",
    "correlations = correlations.dropna()\n",
    "correlations = correlations.sort_values()\n",
    "\n",
    "# --- Step 2: Subset the Top 10 Negative and Top 10 Positive Correlations ---\n",
    "top10_negative = correlations.head(10)\n",
    "top10_positive = correlations.tail(10)\n",
    "\n",
    "# Combine the two subsets into one Series and sort again.\n",
    "subset_correlations = pd.concat([top10_negative, top10_positive]).sort_values()\n",
    "\n",
    "# --- Step 3: Map Feature Names to Descriptive Labels ---\n",
    "# Define a mapping dictionary for your new features.\n",
    "feature_mapping = {\n",
    "    \"AA.comp_AA.Comp_I\": \"AAC - Isoleucine\",\n",
    "    \"abundance_HPA_respiratory\": \"Protein level (respiratory)\",\n",
    "    \"AA.comp_AA.Comp_V\": \"AAC - Valine\",\n",
    "    \"abundance_HPA_GI_tract\": \"Protein level (GI tract)\",\n",
    "    \"abundance_HPA_brain\": \"Protein level (brain)\",\n",
    "    \"CTDC_CTDC.secondarystruct.Group2\": \"Secondary structure: Group2\",\n",
    "    \"abundance_GTEx_Artery\": \"Gene expression (artery)\",\n",
    "    \"abundance_HPA_endocrine\": \"Protein level (endocrine)\",\n",
    "    \"AA.comp_AA.Comp_N\": \"AAC - Asparagine\",\n",
    "    \"iPTMnet_Sumoylation\": \"Sumoylation\",\n",
    "    \"PSIM_OG\": \"O-linked glycosylation\",\n",
    "    \"AA.comp_AA.Comp_L\": \"AAC - Leucine\",\n",
    "    \"abundance_PrEST_conc_cf\": \"Production yield in yeast\",\n",
    "    \"CTDT_CTDT.prop6.Tr1331\": \"Transition secondary structure: helix/strand\",\n",
    "    \"PSIM_DSB\": \"Disulfide bond\",\n",
    "    \"CTDT_CTDT.prop5.Tr1331\": \"Transition solvent accessibility: buried/intermediate\"\n",
    "}\n",
    "\n",
    "def map_feature_name(feature):\n",
    "    # Remove the \"sequence_\" prefix if present.\n",
    "    cleaned = feature.replace('sequence_', '')\n",
    "    # Also remove a leading \"protr.\" if present.\n",
    "    if cleaned.startswith(\"protr.\"):\n",
    "        cleaned = cleaned[len(\"protr.\"):]\n",
    "    # Check if any mapping key is a substring of the cleaned feature name.\n",
    "    for key, descriptive in feature_mapping.items():\n",
    "        if key in cleaned:\n",
    "            return descriptive\n",
    "    # If no mapping is found, return the cleaned feature name.\n",
    "    return cleaned\n",
    "\n",
    "# The subset_correlations index contains the original feature names.\n",
    "mapped_index = [map_feature_name(feat) for feat in subset_correlations.index]\n",
    "subset_correlations.index = mapped_index\n",
    "\n",
    "# --- Step 4: Define Colors and Update the Plot ---\n",
    "# Define new colors: royalblue for positive correlations and orange for negative.\n",
    "positive_color = 'royalblue'\n",
    "negative_color = 'orange'\n",
    "colors = [positive_color if val > 0 else negative_color for val in subset_correlations]\n",
    "\n",
    "# --- Step 5: Plot the Correlation Analysis with Improved Aesthetics ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = subset_correlations.plot(kind='barh', color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax.set_title(\"Top 10 Negative and Top 10 Positive Pearson Correlations\\nwith Cell Viability\", fontsize=18, weight='bold')\n",
    "ax.set_xlabel(\"Correlation Coefficient\", fontsize=16)\n",
    "ax.set_ylabel(\"Feature\", fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.xaxis.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "ax.yaxis.grid(False)\n",
    "sns.despine(ax=ax, top=True, right=True)\n",
    "\n",
    "# --- Step 6: Add a Custom Legend ---\n",
    "positive_patch = mpatches.Patch(color=positive_color, label='Positive correlation with viability')\n",
    "negative_patch = mpatches.Patch(color=negative_color, label='Negative correlation with viability')\n",
    "plt.legend(handles=[positive_patch, negative_patch], loc='lower right', fontsize=14)\n",
    "\n",
    "plt.savefig(\"results/corr_viability_protfeatures.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e04f3-257d-46d8-bd43-b137523d36fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
