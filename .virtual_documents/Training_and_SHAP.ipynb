


import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import shap

import xgboost as xgb
from sklearn.metrics import accuracy_score

from interpret.glassbox import ExplainableBoostingClassifier
from interpret import show
from interpret.provider import InlineProvider
from interpret import set_visualize_provider
from interpret.perf import ROC


def new_evaluate(model, X, y):
    proba_preds = model.predict_proba(X)[:,1]
    preds = model.predict(X)
    pred_labels = np.rint(preds)

    import sklearn.metrics as metrics
    # calculate the fpr and tpr for all thresholds of the classification
    fpr, tpr, threshold = metrics.roc_curve(y, proba_preds)
    # calculate AUC
    roc_auc = metrics.auc(fpr, tpr)
    # calculate accuracy
    accuracy = metrics.accuracy_score(y, pred_labels)

    print(f"AUC={roc_auc:.3f}")
    print(f"Accuracy={accuracy:.3f}")
          
    #plot a confusion matrix
    cm = metrics.confusion_matrix(y, pred_labels)

    # plot a confusion matrix as a heatmap
    import matplotlib.pyplot as plt
    import seaborn as sns
    # label axes as "predicted and "actual"
    ax = sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')#, center=0, linewidths=0.5)
    ax.set(xlabel='Predicted', ylabel='Actual')
    plt.show()

    # print the classification report
    print(metrics.classification_report(y, pred_labels))

    # plot the ROC curve
    # label axes
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    #add grid
    plt.grid(True)
    # plot the ROC curve
    plt.plot(fpr, tpr, label=f"AUC={roc_auc:.3f}")
    # plot the 45 degree line
    plt.plot([0,1], [0,1], 'k--', label="Random")
    # add a legend
    plt.legend()
    # show the plot
    #plt.show()



### Load preprocessed data

train_df = pd.read_csv('preprocessed/train.csv')
val_df = pd.read_csv('preprocessed/val.csv')
cal_df = pd.read_csv('preprocessed/cal.csv')
test_df = pd.read_csv('preprocessed/test.csv')

# Load predictor_list
with open('preprocessed/predictor_list.json', 'r') as f:
    predictor_list = json.load(f)





# Load the best parameters from the JSON file
with open("best_params.json", "r") as f:
    best_params = json.load(f)

print("Loaded best parameters:")
for key, value in best_params.items():
    print("    {}: {}".format(key, value))


%%time

# Set the parameters for the best XGBoost model
param = {
    "objective": "binary:logistic",
    "colsample_bytree": best_params["xgb_colsample_bytree"],
    "learning_rate": best_params["xgb_learning_rate"],
    "max_depth": best_params["xgb_max_depth"],
    "n_estimators": best_params["xgb_n_estimators"],
    "subsample": best_params["xgb_subsample"],
    "early_stopping_rounds": 100,
}

# Initialize the XGBoost model with the best parameters
gbm = xgb.XGBClassifier(**param)

# Train the model on the entire training set
gbm.fit(train_df[predictor_list], train_df['Status'], eval_set=[(train_df[predictor_list], train_df['Status'])], verbose=False)

# Evaluate the model on the validation set
preds = gbm.predict(val_df[predictor_list].values)
pred_labels = np.rint(preds).astype(int)
accuracy = accuracy_score(val_df['Status'].values.astype(int), pred_labels)
print(f"XGBoost model accuracy={accuracy:0.3f}")


print("GBM")
new_evaluate(gbm, val_df[predictor_list].values, val_df['Status'].values)





# Create an explainer object that can compute SHAP values for the model

explainer = shap.TreeExplainer(gbm)
shap_values = explainer(test_df[predictor_list])


##################
# BAR PLOT
##################

# Step 1: Separate SHAP values by class
labels = test_df['Status']
shap_values_0 = shap_values.values[labels == 0]  # SHAP values for class 0
shap_values_1 = shap_values.values[labels == 1]  # SHAP values for class 1

# Step 2: Compute mean absolute SHAP values for each feature per class
features = test_df[predictor_list].columns
mean_shap_0 = np.abs(shap_values_0).mean(axis=0)
mean_shap_1 = np.abs(shap_values_1).mean(axis=0)

# Create a DataFrame for plotting
shap_df = pd.DataFrame({
    'Feature': features,
    'Class 0': mean_shap_0,
    'Class 1': mean_shap_1
}).sort_values(by=['Class 0', 'Class 1'], ascending=False)

# Select the top 20 features
shap_df = shap_df.head(16)
# Remove the feature 'iPTMnet_N.Glycosylation' from the dataset
shap_df = shap_df[shap_df['Feature'] != 'sequence_iPTMnet_N.Glycosylation']

# Step 3 Renaming for better visualization
shap_df['Feature'] = shap_df['Feature'].str.replace('sequence_', '', regex=False)
feature_mapping = {
    'PSIM_DSB': 'Disulfide bond',
    'Peptides.R_charge_charge': 'Protein charge',
    'PSIM_NG': 'N-linked glycosylation',
    'PSIM_SP': 'Signal peptide',
    'Mol.Weight_MW..Da.': 'Molecular weight',
    'AA.comp_AA.Comp_R': 'AAC - Arginine',
    'protr.CTDC_CTDC.normwaalsvolume.Group2': 'Van der Waals volume',
    'AA.comp_AA.Comp_T': 'AAC - Threonine',
    'iPTMnet_Phosphorylation': 'Phosphorylation site',
    'iPTMnet_N.Glycosylation': 'N-linked glycosylation',
    'PSIM_TMD': 'Transmembrane domain',
    'protr.CTDC_CTDC.solventaccess.Group3': 'Solvent accessibility',
    'AA.comp_AA.Comp_H': 'AAC - Histidine',
    'abundance_HPA.tissue.mean_HPA.tpm': 'Tissue mean TPM (HPA)',
    'Peptides.R_aaComp.mole_Basic_Mole%': 'Percent basic residues',
    'Peptides.R_aaComp.mole_Aliphatic_Mole%': 'Percent aliphatic residues',
}
shap_df['Feature'] = shap_df['Feature'].replace(feature_mapping)

# Step 4: Create a horizontal grouped bar plot
plt.figure(figsize=(12, 10))
width = 0.35  # Bar width
y = np.arange(len(shap_df))  # Indices for the features

plt.barh(y - width / 2, shap_df['Class 0'], height=width, label='Failed to Secrete', color='#1f77b4', edgecolor='black', linewidth=0.5)
plt.barh(y + width / 2, shap_df['Class 1'], height=width, label='Secreted', color='#ff7f0e', edgecolor='black', linewidth=0.5)

plt.grid(axis='x', linestyle='--', alpha=0.7)

plt.ylabel('Features', fontsize=18, labelpad=10)
plt.xlabel('Mean |SHAP Value|', fontsize=18, labelpad=10)
plt.title('Top 20 SHAP Feature Importance by Class', fontsize=18, pad=10)
plt.yticks(y, shap_df['Feature'], fontsize=16)
plt.xticks(fontsize=14)
plt.legend(fontsize=16, frameon=False, loc='upper center', bbox_to_anchor=(0.8, 0.7), ncol=1)

plt.ylim(-0.5, len(shap_df) - 0.5)
plt.gca().invert_yaxis()

plt.tight_layout()
plt.savefig('results/shap_feature_importance.svg', format='svg', bbox_inches='tight') 
plt.show()





#########################
# SWARM PLOT
#########################

# Step 1: Filter SHAP values and test data for each class
shap_values_0 = shap_values.values[labels == 0]  # SHAP values for Class 0
shap_values_1 = shap_values.values[labels == 1]  # SHAP values for Class 1

test_df_0 = test_df[labels == 0]  # Data corresponding to Class 0
test_df_1 = test_df[labels == 1]  # Data corresponding to Class 1


# Step 2:Generate renamed feature names
renamed_feature_names = [
    feature_mapping.get(col.replace('sequence_', ''), col.replace('sequence_', ''))
    for col in predictor_list
]


# Step 2: Create a SHAP summary plot for Class 0
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values_0, 
    test_df_0[predictor_list], 
    feature_names=renamed_feature_names,
    plot_type="dot", 
    show=False, 
    max_display=15
)

plt.gca().tick_params(axis='y', labelsize=14)
plt.gca().set_xlabel("SHAP Value (impact on model output)", fontsize=14, labelpad=10)
plt.savefig('results/shap_swarmplot_class_0.svg', format='svg', bbox_inches='tight')
plt.show()

# Step 3: Create a SHAP summary plot for Class 1
plt.figure(figsize=(12, 8))
shap.summary_plot(
    shap_values_1, 
    test_df_1[predictor_list],
    feature_names=renamed_feature_names,
    plot_type="dot", 
    show=False, 
    max_display=15
)
plt.gca().tick_params(axis='y', labelsize=14)
plt.gca().set_xlabel("SHAP Value (impact on model output)", fontsize=14, labelpad=10)
plt.savefig('results/shap_swarmplot_class_1.svg', format='svg', bbox_inches='tight')
plt.show()








# Step 1: Identify a representative sample index for each class
# Example: Use the first occurrence of each class as a representative sample
sample_idx_0 = labels[labels == 0].index[20]  # First instance of Class 0
sample_idx_1 = labels[labels == 1].index[0]  # First instance of Class 1

# Step 2: Generate force plot for Class 0 representative sample
shap.force_plot(
    base_value=explainer.expected_value,            # The base value (average model output)
    shap_values=shap_values.values[sample_idx_0],   # SHAP values for this sample
    features=test_df[predictor_list].iloc[sample_idx_0],  # Feature values for this sample
    feature_names=predictor_list,
    matplotlib=True  # Render inline with matplotlib
)

# Step 3: Generate force plot for Class 1 representative sample
shap.force_plot(
    base_value=explainer.expected_value,            # The base value (average model output)
    shap_values=shap_values.values[sample_idx_1],   # SHAP values for this sample
    features=test_df[predictor_list].iloc[sample_idx_1],  # Feature values for this sample
    feature_names=predictor_list,
    matplotlib=True  # Render inline with matplotlib
)





test_df


# Step 1: Add Status temporarily to the feature matrix
test_df_with_status = test_df[predictor_list].copy()
test_df_with_status['Status'] = test_df['Status']  # Add Status to the feature matrix

# Step 2: Add an extra column to SHAP values
# Create a column of zeros for the 'Status' feature
status_shap_column = np.zeros((shap_values.values.shape[0], 1))

# Append the new column to the SHAP values array
shap_values_with_status = np.hstack([shap_values.values, status_shap_column])

# Step 3: Dependence plot for a specific feature with the target variable as the color
feature_name = 'sequence_PSIM_DSB'  # Replace with your feature of interest

shap.dependence_plot(
    feature_name,
    shap_values_with_status,      # Modified SHAP values with the Status column
    test_df_with_status,          # Feature matrix including Status
    interaction_index='Status'    # Use Status for coloring
)





# Step 1: Filter SHAP values and test data for each class
shap_values_0 = shap_values.values[labels == 0]  # SHAP values for Class 0
shap_values_1 = shap_values.values[labels == 1]  # SHAP values for Class 1

test_df_0 = test_df[labels == 0][predictor_list]  # Features for Class 0
test_df_1 = test_df[labels == 1][predictor_list]  # Features for Class 1

# Step 2: Generate decision plot for all samples of Class 0
shap.decision_plot(
    base_value=explainer.expected_value,       # The base value (average model output)
    shap_values=shap_values_0,                 # SHAP values for all Class 0 samples
    feature_names=predictor_list,              # Feature names
    link='identity'                            # Direct scale for SHAP values
)

# Step 3: Generate decision plot for all samples of Class 1
shap.decision_plot(
    base_value=explainer.expected_value,       # The base value (average model output)
    shap_values=shap_values_1,                 # SHAP values for all Class 1 samples
    feature_names=predictor_list,              # Feature names
    link='identity'                            # Direct scale for SHAP values
)



shap.decision_plot(explainer.expected_value, explainer.shap_values(val_df[predictor_list].values), val_df[predictor_list], feature_names=predictor_list)


shap.plots.heatmap(shap_values, instance_order=shap_values.sum(1))





ebm = ExplainableBoostingClassifier()
ebm.fit(train_df[predictor_list], train_df['Status'])


set_visualize_provider(InlineProvider())


# create ebm_global with the top 40 features
ebm_global = ebm.explain_global(  )#name='EBM')
show(ebm_global)


ebm_perf = ROC(ebm).explain_perf(val_df[predictor_list], val_df['Status'], name='EBM')
show(ebm_perf)


print("EBM")
new_evaluate(ebm, val_df[predictor_list].values, val_df['Status'].values)
