





import json
import pandas as pd
import numpy as np
import missingno as msno

from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# ML
import optuna
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Neural Network Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

# Plot
import seaborn as sns
import matplotlib.pyplot as plt


# Functions

def show_heatmap(df, predictor_list, target = 'Status', figsize = (20,10), title='Correlation Heatmap'):
    # create a correlation matrix but specify numeric_only=True to only include columns with numeric data
    correlation_matrix = df[predictor_list + [target]].corr(method='pearson', min_periods=1)
    # create a mask for the upper triangle
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    # set up the matplotlib figure
    plt.figure(figsize=figsize)
    # add a title
    plt.title(title)
    # plot the heatmap
    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=".2f", cmap='coolwarm', center=0, linewidths=0.5)
    plt.show()

def standardize_values(X):
    from sklearn.preprocessing import StandardScaler
    # create a standard scaler object
    scaler = StandardScaler()
    # fit the scaler to the data
    scaler.fit(X)
    # transform the data
    X_stand = scaler.transform(X)
    return X_stand

def t_sne_plot(X, y, title='t-SNE plot'):
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    # create a t-SNE object
    tsne = TSNE(n_components=2, random_state=0)
    # fit the t-SNE object to the data
    X_tsne = tsne.fit_transform(X)
    # create a scatter plot of the t-SNE data
    plt.figure(figsize=(10,10))
    plt.scatter(X_tsne[y==0, 0], X_tsne[y==0, 1], label='Fail', c='red', alpha=0.5)
    plt.scatter(X_tsne[y==1, 0], X_tsne[y==1, 1], label='Pass', c='blue', alpha=0.5)
    plt.title(title)
    plt.legend()
    plt.show()





# read in the sheet called 'all features' from the raw/media-4.xlsx excel file

df = pd.read_excel('data/media-4.xlsx', sheet_name='all features')
target_df = pd.read_excel('data/media-12.xlsx')

# drop unnecessary columns
df = df.drop(columns=['sample_ID','uniprot_id'])
target_df = target_df[['Gene.name', 'Status']]

# Rename column in the target for merging
target_df = target_df.rename(columns={'Gene.name':'human_symbol'})

# Set Index
df.set_index('human_symbol', inplace=True)
target_df.set_index('human_symbol', inplace=True)

# Group by index and calculate the mean for each group (this will erase duplicate genes)
df = df.groupby(df.index).mean()


target_df





# Missing values
msno.bar(df,figsize=(12, 48), sort="ascending",fontsize=12, color='tomato')


# Drop columns with more than 50% of missing values

# Define the threshold
threshold = 0.5

# Calculate the percentage of missing values for each column
missing_percentage = df.isnull().mean()

# Drop columns exceeding the threshold
columns_to_drop = missing_percentage[missing_percentage > threshold].index
df_dropped = df.drop(columns=columns_to_drop)


# Use KNN inputer to inpute the rest of the missing values

# Save the original index
original_index = df_dropped.index

imputer = KNNImputer(n_neighbors=5)
df_imputed_array = imputer.fit_transform(df_dropped)

df_imputed = pd.DataFrame(df_imputed_array, columns=df_dropped.columns, index=original_index)


# Replace 'Pass' with 1 and anything else with 0 in the target_df

target_df.loc[:,'Status'] = target_df['Status'].replace({'Pass':1})
# if 'Status' != 1, replace with 0
target_df.loc[(target_df['Status']!=1),'Status'] = 0
# display unique classes in 'Status' column
display(target_df['Status'].groupby(target_df['Status']).count())
target_df.head()


# subsample tartet_df['Status'] == 1 to 895
pass_df = target_df[target_df['Status'] == 1].sample(n=895, random_state=1)
fail_df = target_df[target_df['Status'] == 0]

# combine pass_df and fail_df
new_target_df = pd.concat([pass_df, fail_df])

print(new_target_df['Status'].groupby(new_target_df['Status']).count())
new_target_df.head()


# Combine features df with Targets df

df_merged = df_imputed.join(new_target_df, how='inner')
print(df_merged.shape)





# find any columns with zero variability
zero_var_cols = df_merged.columns[df_merged.nunique() == 1]
print(zero_var_cols)





# Define correlation threshold
correlation_threshold = 0.80

# Create correlation matrix (numeric_only=True to only include numeric data)
correlation_matrix = df_merged.corr(method='pearson')

# Select upper triangle of correlation matrix
upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Find index of feature columns with correlation greater than correlation_threshold
to_drop = [column for column in upper.columns if any(abs(upper[column]) > correlation_threshold)]

# Print columns to drop and their highly correlated counterparts
for column in to_drop:
    print(f"Column {column} is highly correlated with {list(upper.index[upper[column] > correlation_threshold])}")

print()
print(f'{len(to_drop)} columns to drop: {to_drop}')





predictor_list = list(df_merged.columns)
predictor_list.remove('Status')

#remove the columns to drop
predictor_list = [x for x in predictor_list if x not in to_drop]
print(len(predictor_list))

# Save to a JSON file
with open('preprocessed/predictor_list.json', 'w') as f:
    json.dump(predictor_list, f)





show_heatmap(df=df_merged, target='Status', predictor_list=predictor_list, title='Correlation Heatmap', figsize=(65,65))





X_stand  = standardize_values(df_merged[predictor_list].values)

# convert X_stand to a dataframe with columns = df.columns
X_stand_df = pd.DataFrame(X_stand, columns=predictor_list)
# set index of X_stand_df to be the same as new_df
X_stand_df.index = df_merged.index
# add 'Status' to X_stand_df
X_stand_df['Status'] = df_merged['Status']
X_stand_df.describe()


show_heatmap(df=X_stand_df, target='Status', predictor_list=predictor_list, title='Correlation Heatmap', figsize=(65,65))





# create a t-sne plot of the data in new_df colored by the 'Status' column
t_sne_plot(X_stand, df_merged['Status'], title='t-SNE plot of X_stand')





df_merged['Status'].astype(int)
df_merged.shape


# Shuffle the rows and split the data into training, validation, calibration, and testing sets

# Step 1: Split the data into training (70%) and temporary (30%) sets
train_df, tmp_df = train_test_split(df_merged, test_size=0.3, random_state=1, stratify=df_merged['Status'])
# train_df: 70% of the data, used for training the model
# tmp_df: 30% of the data, used for further splitting into validation and testing sets

# Step 2: Split the temporary set into validation (20% of the original) and testing (10% of the original) sets
tmp2_df, test_df = train_test_split(tmp_df, test_size=0.33, random_state=1, stratify=tmp_df['Status'])
# tmp2_df: ~20% of the original data, used for further splitting into validation and calibration sets
# test_df: ~10% of the original data, used for final model testing

# Step 3: Split the tmp2_df data into validation (10% of the original) and calibration (10% of the original) sets
val_df, cal_df = train_test_split(tmp2_df, test_size=0.5, random_state=1, stratify=tmp2_df['Status'])
# val_df: ~10% of the original data, used for model validation during training
# cal_df: ~10% of the original data, used for model calibration or hyperparameter tuning

# Display the shapes of the resulting datasets
train_df.shape, val_df.shape, cal_df.shape, test_df.shape


# check the distribution of 'Status' in train_df, val_df, cal_df, and test_df
display(train_df['Status'].groupby(train_df['Status']).count())
display(val_df['Status'].groupby(val_df['Status']).count())
display(cal_df['Status'].groupby(cal_df['Status']).count())
display(test_df['Status'].groupby(test_df['Status']).count())



# Save datasets in the "preprocessed" folder
train_df.to_csv('preprocessed/train.csv', index=False)
val_df.to_csv('preprocessed/val.csv', index=False)
cal_df.to_csv('preprocessed/cal.csv', index=False)
test_df.to_csv('preprocessed/test.csv', index=False)







def get_data():
    y = train_df['Status'].values.astype(int)  # Ensure the target values are integers
    X = train_df[predictor_list].values
    return X, y

# Objective function for Optuna
def objective(trial: optuna.Trial) -> float:
    X, y = get_data()
    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25, stratify=y)
    
    # Suggest classifier
    classifier_name = trial.suggest_categorical("classifier", ["XGBoost", "RandomForest", "LogisticRegression", "NeuralNetwork"])
        
    if classifier_name == "XGBoost":
        param = {
            "objective": "binary:logistic",
            "colsample_bytree": trial.suggest_float("xgb_colsample_bytree", 0.5, 1.0),
            "learning_rate": trial.suggest_float("xgb_learning_rate", 0.01, 0.1),
            "max_depth": trial.suggest_int("xgb_max_depth", 1, 12),
            "n_estimators": trial.suggest_int("xgb_n_estimators", 100, 1000),
            "subsample": trial.suggest_float("xgb_subsample", 0.5, 1.0),
            "early_stopping_rounds": 100,
        }
        gbm = xgb.XGBClassifier(**param)
        gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=False)
        preds = gbm.predict(valid_x)
        
    elif classifier_name == "RandomForest":
        param = {
            "n_estimators": trial.suggest_int("rf_n_estimators", 100, 1000),
            "max_depth": trial.suggest_int("rf_max_depth", 1, 12),
            "min_samples_split": trial.suggest_int("rf_min_samples_split", 2, 10),
            "min_samples_leaf": trial.suggest_int("rf_min_samples_leaf", 1, 10),
            "bootstrap": trial.suggest_categorical("rf_bootstrap", [True, False]),
        }
        gbm = RandomForestClassifier(**param)
        gbm.fit(train_x, train_y)
        preds = gbm.predict(valid_x)
        
    elif classifier_name == "LogisticRegression":
        param = {
            "C": trial.suggest_float("lr_C", 1e-6, 1e2, log=True),
            "max_iter": trial.suggest_int("lr_max_iter", 100, 1000),
            "solver": trial.suggest_categorical("lr_solver", ["lbfgs", "liblinear", "sag", "saga"]),
        }
        gbm = LogisticRegression(**param)
        gbm.fit(train_x, train_y)
        preds = gbm.predict(valid_x)

    elif classifier_name == "NeuralNetwork":
        # Define hyperparameters
        n_layers = trial.suggest_int("nn_n_layers", 2, 4)
        dropout_rate = trial.suggest_float("nn_dropout_rate", 0.3, 0.5)
        layer_size = trial.suggest_int("nn_layer_size", 64, 256)
        learning_rate = trial.suggest_float("nn_learning_rate", 1e-5, 1e-3, log=True)
        
        # Build neural network
        model = Sequential()
        model.add(Dense(layer_size, activation="relu", input_shape=(train_x.shape[1],)))
        
        for _ in range(n_layers):
            model.add(Dense(layer_size, activation="relu"))
            model.add(Dropout(dropout_rate))
            layer_size = max(layer_size // 2, 32)  # Decrease layer size progressively
            
        model.add(Dense(1, activation="sigmoid"))
        
        # Compile model
        model.compile(optimizer=Adam(learning_rate=learning_rate), loss="binary_crossentropy", metrics=["accuracy"])
        
        # Train model
        model.fit(train_x, train_y, epochs=trial.suggest_int("nn_epochs", 50, 200), batch_size=trial.suggest_int("nn_batch_size", 32, 256), verbose=0, validation_data=(valid_x, valid_y))
        preds = (model.predict(valid_x) > 0.5).astype(int).flatten()


    pred_labels = np.rint(preds).astype(int) # Ensure binary format (0 or 1)
    valid_y = valid_y.astype(int)
    accuracy = accuracy_score(valid_y, pred_labels)
    return accuracy


# Optuna Study

import warnings
warnings.filterwarnings("ignore")

study = optuna.create_study(
    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction="maximize"
)
study.optimize(objective, n_trials=500, timeout=3600)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

# Save the best parameters to a JSON file
best_params = trial.params
with open("best_params.json", "w") as f:
    json.dump(best_params, f, indent=4)

print("Best parameters saved to best_params.json")
